{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Lenght Measurements\n",
    "\n",
    "### Authors: \n",
    "- Troels C. Petersen (Niels Bohr Institute)\n",
    "- Christian Michelsen (Niels Bohr Institute)\n",
    "\n",
    "\n",
    "### Date:    \n",
    "- 19-11-2019 (latest update)\n",
    "\n",
    "***\n",
    "\n",
    "Python program for analysing measurements of the length of the lecture table in Auditorium A at NBI.  \n",
    "There are two measurements each with estimated error of the table length:\n",
    "1. Measurement with a 30cm ruler.\n",
    "2. Measurement with a 2m folding ruler.\n",
    "\n",
    "Each person was asked not only to state the measurement, but also their estimated uncertainty. None of the persons could see others measurements in order to get the largest degree of independence. Also, the 30cm ruler measurement was asked to be done first.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import the modules you want to use:\n",
    "import numpy as np                                     # Matlab like syntax for linear algebra and functions\n",
    "import matplotlib.pyplot as plt                        # Plots and figures like you know them from Matlab\n",
    "from iminuit import Minuit                             # The actual fitting tool, better than scipy's\n",
    "import sys\n",
    "from scipy import stats\n",
    "from scipy.special import erfc                         # Error function, to get integral of Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add out our custom functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../External_Functions')\n",
    "from ExternalFunctions import Chi2Regression, BinnedLH, UnbinnedLH\n",
    "from ExternalFunctions import nice_string_output, add_text_to_ax # useful functions to print fit results on figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for the program: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blinded = True\n",
    "apply_chauvenets_criterion = True\n",
    "save_plots = False\n",
    "\n",
    "r = np.random          # Random numbers\n",
    "r.seed(42)             # We set the numbers to be random, but the same for each run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions:\n",
    "\n",
    "Function to find the bin centers and counts in a range from `xmin` to `xmax` of a histogram `hist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_bincenter_and_counts_in_range(hist, xmin=None, xmax=None):\n",
    "    \n",
    "    if xmin is None:\n",
    "        xmin = np.min(hist)\n",
    "    if xmax is None:\n",
    "        xmax = np.max(hist)\n",
    "    \n",
    "    counts, bin_edges, _ = hist\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    mask1 = (xmin < bin_centers) & (bin_centers <= xmax) \n",
    "    mask2 = counts > 0\n",
    "    mask_final = mask1 & mask2\n",
    "    return bin_centers[mask_final], counts[mask_final], np.sqrt(counts[mask_final])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the $\\chi^2$-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chi2(function, x_values, y_values, sy_values, *fitparameters):\n",
    "    # traditional loop-version\n",
    "    chi2_val = 0\n",
    "    entries = 0\n",
    "    for x, y, sy in zip(x_values, y_values, sy_values):\n",
    "        if y > 0:\n",
    "            f = function(x, *fitparameters) # calc the model value\n",
    "            residual  = ( y-f ) / sy  # find the uncertainty-weighted residual\n",
    "            chi2_val += residual**2  # the chi2-value is the squared residual\n",
    "            entries += 1 # count the bin as non-empty since sy>0 (and thus y>0)\n",
    "    \n",
    "    # numpy version\n",
    "    mask = (y_values>0)\n",
    "    yhat = function(x_values, *fitparameters)\n",
    "    chi2_val = np.sum( (y_values[mask]-yhat[mask])**2/sy_values[mask]**2)\n",
    "    entries = sum(mask)\n",
    "            \n",
    "    return chi2_val, entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian PDF (unit integral):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your PDF / model \n",
    "def gauss_pdf(x, mu, sigma):\n",
    "    \"\"\"Normalized Gaussian\"\"\"\n",
    "    return 1 / np.sqrt(2 * np.pi) / sigma * np.exp(-(x - mu) ** 2 / 2. / sigma ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extended Gaussian PDF (including normalisation, which enables fit to histograms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def gauss_extended(x, N, mu, sigma) :\n",
    "    \"\"\"Non-normalized Gaussian\"\"\"\n",
    "    return N * gauss_pdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data analysis:\n",
    "\n",
    "Before we even look at the look at the data, we decide whether or not we want to blind the analysis by adding a constat to all measurements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if blinded:\n",
    "    blinding = r.normal(0, 0.1)      # I add a constant (Gaussian with +-10cm) to remain \"blind\"\n",
    "else:\n",
    "    blinding = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what datafiles we want to look at. Extend it to suit your analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiles = [\"data_TableMeasurements2018.txt\",\n",
    "           \"data_TableMeasurements2017.txt\",\n",
    "           \"data_TableMeasurements2016.txt\",\n",
    "           \"data_TableMeasurements2015.txt\",\n",
    "           \"data_TableMeasurements2014.txt\",\n",
    "           \"data_TableMeasurements2013.txt\",\n",
    "           \"data_TableMeasurements2012.txt\",\n",
    "           \"data_TableMeasurements2011.txt\",\n",
    "           \"data_TableMeasurements2010.txt\",\n",
    "           \"data_TableMeasurements2009.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in all the data from the `infiles` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L30cm = np.array([])\n",
    "eL30cm = np.array([])\n",
    "L2m = np.array([])\n",
    "eL2m = np.array([])\n",
    "\n",
    "# Loop over files and open them\n",
    "for infile in infiles:\n",
    "    \n",
    "    tmp_L30cm, tmp_eL30cm, tmp_L2m, tmp_eL2m = np.loadtxt(infile, skiprows=2, unpack=True)\n",
    "    \n",
    "    L30cm = np.append(L30cm, tmp_L30cm + blinding)\n",
    "    eL30cm = np.append(eL30cm, tmp_eL30cm)\n",
    "    L2m = np.append(L2m, tmp_L2m + blinding)\n",
    "    eL2m = np.append(eL2m, tmp_eL2m)\n",
    "    \n",
    "N_read  = len(L30cm)       # Number of measurements read in total\n",
    "\n",
    "\n",
    "print(f\"\\n\\nRead all {len(infiles)} file(s) which included {N_read} measurements. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minL = 0.0\n",
    "maxL = 5.0\n",
    "\n",
    "## Define two histograms with all the lengths recorded:\n",
    "fig_raw, ax = plt.subplots(nrows=2, figsize=(14,8))\n",
    "ax_L30cm, ax_L2m = ax\n",
    "\n",
    "hist_L30cm = ax_L30cm.hist(L30cm, bins=1000, range=(minL, maxL), histtype='step', label='Binned Data')\n",
    "ax_L30cm.set(xlabel='Table lenght (m)', ylabel='Frequency', title='Lengths estimates by 30cm ruler')\n",
    "\n",
    "hist_L2m = ax_L2m.hist(L2m, bins=1000, range=(minL, maxL), histtype='step', label='Binned Data')\n",
    "ax_L2m.set(xlabel='Table lenght (m)', ylabel='Frequency', title='Lengths estimates by 2m ruler')\n",
    "\n",
    "fig_raw.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 cm ruler:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the measurements for the 30cm ruler and focus on that for now. Below is a mean and RMS calculation along with a general Gaussian fit to all the data. Somehow, it doesn't seem optimal/right..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  Initial estimate (30 cm ruler):  {L30cm.mean():.3f} +- {L30cm.std(ddof=1)/np.sqrt(len(L30cm)):.3f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the fit. Can you guess why we use the binwidth `L30cm_binwidth` in the inital value for `N`in the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nL30cm raw fit: \\n\")\n",
    "\n",
    "L30cm_x, L30cm_y, L30cm_sy = get_bincenter_and_counts_in_range(hist_L30cm, 3.0, 3.7)\n",
    "L30cm_binwidth = L30cm_x[1] - L30cm_x[0]\n",
    "\n",
    "chi2_L30cm = Chi2Regression(gauss_extended, L30cm_x, L30cm_y, L30cm_sy) \n",
    "minuit_L30cm = Minuit(chi2_L30cm, pedantic=False, N=L30cm_y.sum()*L30cm_binwidth, mu=L30cm.mean(), sigma=L30cm.std(ddof=1)) \n",
    "minuit_L30cm.migrad();\n",
    "\n",
    "# the fitted values of the parameters\n",
    "L30cm_fit_N, L30cm_fit_mu, L30cm_fit_sigma = minuit_L30cm.args "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the fit on the figure and extract relevant extra information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the x-axis for the plot of the fitted function\n",
    "xaxis = np.linspace(minL, maxL, 10000) \n",
    "\n",
    "# Compute the fitted function for x_fit\n",
    "yaxis = gauss_extended(xaxis, *minuit_L30cm.args) \n",
    "\n",
    "# Plot the fitted function\n",
    "ax_L30cm.plot(xaxis, yaxis, '-', label='Fit')\n",
    "\n",
    "# Compute the chi2, the number of non-empty bins, the NDOF and the chi2-probability\n",
    "L30cm_chi2, L30cm_entries = calculate_chi2(gauss_extended, L30cm_x, L30cm_y, L30cm_sy, *minuit_L30cm.args)\n",
    "L30cm_NDOF = L30cm_entries - len(minuit_L30cm.args)\n",
    "L30cm_chi2_prob =  stats.chi2.sf(L30cm_chi2, L30cm_NDOF) \n",
    "\n",
    "\n",
    "# make the results ready to be plotted\n",
    "d = {'Entries': len(L30cm), \n",
    "     'Mean': L30cm.mean(), \n",
    "     'Std': L30cm.std(ddof=1), \n",
    "     'Chi2': L30cm_chi2, \n",
    "     'ndf': L30cm_NDOF, \n",
    "     'Prob': L30cm_chi2_prob, \n",
    "    }\n",
    "for name in minuit_L30cm.parameters:\n",
    "    d[name] = [minuit_L30cm.values[name], minuit_L30cm.errors[name]]\n",
    "\n",
    "# add these results to the plot \n",
    "text = nice_string_output(d, extra_spacing=2, decimals=4)\n",
    "add_text_to_ax(0.02, 0.95, text, ax_L30cm, fontsize=12)\n",
    "\n",
    "ax_L30cm.legend()\n",
    "\n",
    "# show the actual fit and fit results\n",
    "fig_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Start by taking a close look at the data, first by inspecting the numbers in the data file (yes, open the damn thing, and look over the numbers by eye!), and then by\n",
    "considering the histograms produced by running the notebook. \n",
    "\n",
    "To begin with, only consider the 30cm ruler measurements, and disregard the estimated/guessed uncertainties. You can then expand from there, as guided below by questions.\n",
    "\n",
    "# Questions:\n",
    "\n",
    "1. Consider the mean and width. Is the result as you would expect it? And do you think that it is close to the best possible (i.e. most accurate and precise) estimate? NOTE: Make sure that you know the difference between accuracy and precision!!! See \"Common definition\" in: http://en.wikipedia.org/wiki/Accuracy_and_precision\n",
    "\n",
    "2. Do any of the measurements looks wrong/bad/suspecious? Do you see any repeated mistakes done for obvious reasons? Would you correct or exclude any of the measurements and how would you justify this? This problem requires that you discuss internally, and then each do what you think most justified/best. Apply this to the list of measurements, and perhaps produce a new list with your accepted measurements in (to save the original data). How many measurements did you throw away in the end?\n",
    "\n",
    "3. Fit your accepted length measurements with a Gaussian distribution, possibly in a (small?) range around what you believe is the true value. Should the binning be changed for these fits? And is the Gaussian distribution justified? Also, do you see any \"human\" effects? Did any of your class mates (or you?) not read to mm precision?\n",
    "\n",
    "4. Once you have selected the measurements you want to use, calculate the mean, RMS and uncertainty on the mean. How much did your result improve in precision?\n",
    "\n",
    "\n",
    "#### Now repeat the above for the 2m folding rule\n",
    " \n",
    "***\n",
    "\n",
    "\n",
    "5. How much better/worse is the single measurement uncertainty from the 30cm ruler case to the 2m folding rule?\n",
    "\n",
    "6. The \"Pull\" distribution is defined as the plot of $z_i = \\left(x_i - \\mu \\right)/\\sigma_i$ where $\\mu$ is the *sample* mean of $x$, and $x_i$ and $\\sigma_i$ are the single measurements and their corresponding uncertainties. If the measurements and uncertainties are good, then it should give a unit Gaussian. Is that the case? And thus, were the uncertainty estimates/guesses reasonable? If not, then the pull standard deviation is often used to remove overly precise measurements, and scale the errors on the remaining measurements to a reasonable level.\n",
    "\n",
    "7. Try to calculate the weighted mean. Did you get a good Chi2 probability, when doing so? Do you need to discard more dubious measurements? Did the result improve further in precision?\n",
    "\n",
    "8. Does the length of the table seems to be different when measured with a 30cm and a 2m ruler? Quantify this statement! I.e. what is the difference, and what is the uncertainty on that difference? Is this significantly away from 0?\n",
    "\n",
    "9. If you were asked for the best estimate of the length of the table, what would you do? (If posssible, read Bevington page 58 bottom!)\n",
    "\n",
    "\n",
    "### Not too advanced questions:\n",
    "10. Consider the 2018 data file with additional information in (Gender and measurement speed), and find out if gender and measurement speed have an impact on the quality of the measurements.\n",
    "\n",
    "### Advanced questions:\n",
    "11. Is there any correlation between the errors on the measurements and the distance value? I.e. do you see any effect of those measuring e.g. too long having a smaller/larger uncertainty? What would the effect of this be?"
   ]
  }
 ],
 "metadata": {
  "executable": "/usr/bin/env python",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
